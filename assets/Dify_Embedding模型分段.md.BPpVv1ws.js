import{_ as i,c as r,o as a,ag as o}from"./chunks/framework.B-XtCDNB.js";const p=JSON.parse('{"title":"Embedding模型分段","description":"","frontmatter":{},"headers":[],"relativePath":"Dify/Embedding模型分段.md","filePath":"Dify/Embedding模型分段.md"}'),n={name:"Dify/Embedding模型分段.md"};function e(t,l,s,d,u,g){return a(),r("div",null,[...l[0]||(l[0]=[o('<h1 id="embedding模型分段" tabindex="-1">Embedding模型分段 <a class="header-anchor" href="#embedding模型分段" aria-label="Permalink to &quot;Embedding模型分段&quot;">​</a></h1><h1 id="gpt答案" tabindex="-1">GPT答案： <a class="header-anchor" href="#gpt答案" aria-label="Permalink to &quot;GPT答案：&quot;">​</a></h1><h1 id="_1-分段设置-chunking" tabindex="-1">1. 分段设置（Chunking） <a class="header-anchor" href="#_1-分段设置-chunking" aria-label="Permalink to &quot;1. 分段设置（Chunking）&quot;">​</a></h1><h3 id="🔹-通用模式" tabindex="-1">🔹 通用模式 <a class="header-anchor" href="#🔹-通用模式" aria-label="Permalink to &quot;🔹 通用模式&quot;">​</a></h3><ul><li>**分段标识符：**​ <strong>​<code>\\n\\n</code>​</strong><br> → 按段落空行分割，一般能得到语义较完整的块。</li><li><strong>最大长度：1024 characters</strong><br> → 单块不要太短，否则缺乏上下文；也不要太长，否则 embedding 失效。<br> → 推荐：<strong>512–1024 字符（约 150–300 汉字）</strong> 。</li><li><strong>重叠长度：50 characters</strong><br> → 保证跨段的上下文不会丢失。</li></ul><p>✅ 适合：普通 FAQ、说明文档、书籍、政策文件。</p><hr><h3 id="🔹-父子分段模式" tabindex="-1">🔹 父子分段模式 <a class="header-anchor" href="#🔹-父子分段模式" aria-label="Permalink to &quot;🔹 父子分段模式&quot;">​</a></h3><ul><li><p><strong>父块</strong> <strong>=</strong> <strong>上下文</strong>，<strong>子块</strong> <strong>=</strong> <strong>检索单位</strong>。</p></li><li><p>举例：</p><ul><li>父块 = 整个章节（2k–4k tokens）</li><li>子块 = 小段落（512 tokens）<br> → 检索时找到子块，但在回答时可以把整个父块作为上下文给模型。</li></ul></li></ul><p>✅ 优点：</p><ul><li>保证上下文完整，不会因为切片太小而“断章取义”。</li><li>减少“答案缺少上下文”的情况。</li></ul><p>⚠️ 缺点：</p><ul><li>如果父块太大（&gt;10k tokens），会被截断。</li></ul><p>✅ 适合：<strong>法规、合同、科研论文</strong>（需要上下文完整性）。</p><hr><h3 id="🔹-q-a-分段" tabindex="-1">🔹 Q&amp;A 分段 <a class="header-anchor" href="#🔹-q-a-分段" aria-label="Permalink to &quot;🔹 Q&amp;A 分段&quot;">​</a></h3><ul><li>自动根据问答对来切块。</li><li>适合已有 FAQ 或常见问答格式的语料。</li></ul><hr><h1 id="_2-文本预处理" tabindex="-1">2. 文本预处理 <a class="header-anchor" href="#_2-文本预处理" aria-label="Permalink to &quot;2. 文本预处理&quot;">​</a></h1><p>推荐保持默认规则：</p><ul><li>去掉多余空格、换行符、制表符</li><li>删除 URL 和邮箱（除非你就是要检索这些）</li></ul><hr><h1 id="_3-索引方式" tabindex="-1">3. 索引方式 <a class="header-anchor" href="#_3-索引方式" aria-label="Permalink to &quot;3. 索引方式&quot;">​</a></h1><ul><li><strong>高质量（Embedding 检索）</strong> ：<br> → 调用 embedding 模型（如 qwen3-embedding:4b），能保证语义相似度。<br> → 缺点是要消耗 token。</li><li><strong>经济（关键词）</strong> ：<br> → 只基于关键词，不用 token，快但效果差。</li></ul><p>✅ 推荐：<strong>生产环境用高质量</strong>，测试/小语料可用经济。</p><hr><h1 id="_4-检索设置" tabindex="-1">4. 检索设置 <a class="header-anchor" href="#_4-检索设置" aria-label="Permalink to &quot;4. 检索设置&quot;">​</a></h1><h3 id="_1-向量检索" tabindex="-1">① 向量检索 <a class="header-anchor" href="#_1-向量检索" aria-label="Permalink to &quot;① 向量检索&quot;">​</a></h3><ul><li>Top K = 3（通常 3–5）</li><li>Score 阈值 = 0.5（低于这个分数的匹配会丢弃）</li><li>Rerank 模型：可以帮助在 Top K 中重新排序，提升精确度。</li></ul><h3 id="_2-全文检索" tabindex="-1">② 全文检索 <a class="header-anchor" href="#_2-全文检索" aria-label="Permalink to &quot;② 全文检索&quot;">​</a></h3><ul><li>直接匹配关键词，适合特定领域（比如法律条款、代码）。</li></ul><h3 id="_3-混合检索-推荐" tabindex="-1">③ 混合检索（推荐） <a class="header-anchor" href="#_3-混合检索-推荐" aria-label="Permalink to &quot;③ 混合检索（推荐）&quot;">​</a></h3><ul><li><p>‍</p></li><li><p>结合向量检索和全文检索，取两者的交集+rerank。</p></li><li><p>对 <strong>专业术语+长文本</strong> 特别有效。</p><ul><li><p><strong>权重融合 (Semantic 0.7 / Keyword 0.3)</strong></p><ul><li><p>先把语义得分和关键词得分按比例融合，选 Top K。</p></li><li><p>优点：</p><ul><li>计算成本低（只做向量搜索 + BM25）。</li><li>对语义和关键词的“比例”可控。</li></ul></li><li><p>缺点：</p><ul><li>融合公式比较粗糙，对长语料和模糊问题时可能失真。</li></ul></li></ul><p>适合场景：</p><ul><li>数据规模不大</li><li>对检索速度敏感（实时性要求高）</li><li>语料结构化明显（FAQ、短文本）</li></ul></li><li><p><strong>Rerank 模型</strong></p><ul><li><p>先取语义/关键词检索 Top N，丢给 <strong>Reranker 模型</strong>，让它逐一判断和问题的相关度。</p></li><li><p>优点：</p><ul><li>Reranker 通常是 <strong>cross-encoder</strong>，直接“读问题 + 文档对”，比 embedding 更准。</li><li>能显著提升排序效果，减少“答非所问”。</li></ul></li><li><p>缺点：</p><ul><li>成本高（每个候选都要过一次大模型/精排模型）。</li><li>如果候选太少（Top K=2），改进有限。</li></ul></li></ul><p>适合场景：</p><ul><li>数据规模较大</li><li>语料复杂（法规、合同、科研文档）</li><li>答案容错率低，必须保证准确性</li></ul></li></ul></li></ul><hr><h1 id="_5-如何保证上下文完整" tabindex="-1">5. 如何保证上下文完整？ <a class="header-anchor" href="#_5-如何保证上下文完整" aria-label="Permalink to &quot;5. 如何保证上下文完整？&quot;">​</a></h1><ol><li><p><strong>分块策略</strong></p><ul><li>普通文档 → 通用模式（512–1024 字符）</li><li>合同/法规/长文档 → 父子分段（子块 512，父块 2k–4k）</li></ul></li><li><p><strong>Top K 调整</strong></p><ul><li>如果模型经常“答不全”，可以从 Top K=3 增加到 5–8。</li></ul></li><li><p><strong>重叠（overlap）</strong></p><ul><li>保证前后语义连续，推荐 50–100 字符。</li></ul></li><li><p><strong>混合检索</strong></p><ul><li>结合向量和关键词，防止错过关键短语。</li></ul></li></ol><hr><h1 id="_6-语料格式最佳实践" tabindex="-1">6. 语料格式最佳实践 <a class="header-anchor" href="#_6-语料格式最佳实践" aria-label="Permalink to &quot;6. 语料格式最佳实践&quot;">​</a></h1><ul><li><strong>推荐格式</strong>：Markdown (<code>.md</code>​)、纯文本 (<code>.txt</code>)、结构化 JSON (Q&amp;A 对)</li><li><strong>避免格式</strong>：复杂的 PDF（带表格/图片）、Word（排版符号多）</li><li><strong>如果是 PDF</strong>：最好先转 Markdown 或纯文本再上传。</li></ul><hr><p>✅ <strong>总结配置推荐</strong></p><ul><li><strong>分段模式</strong>：父子分段（子块 512，父块 2048）</li><li><strong>索引</strong>：高质量（Embedding）</li><li><strong>检索</strong>：混合检索 + rerank，Top K = 5，Score ≥ 0.5</li><li><strong>语料</strong>：Markdown/纯文本，保持段落结构</li></ul>',42)])])}const c=i(n,[["render",e]]);export{p as __pageData,c as default};
